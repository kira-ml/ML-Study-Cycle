"""
ğŸ® CONDITION NUMBER SIMULATOR: ML's "Butterfly Effect" Detector

Think of condition number as the "drama amplifier" of your ML models!
High condition number = Small data changes cause BIG prediction drama. ğŸ˜±

Created by: @kira-ml (GitHub ML Student)
#MachineLearning #DataScience #NumericalStability #MathForML
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.linalg import hilbert

def compute_condition_number_demo():
    """
    ğŸ­ THE CONDITION NUMBER SHOWDOWN:
    Good Matrix vs. Drama Queen Matrix ğŸ¯
    
    CONDITION NUMBER TL;DR:
    â€¢ Low (< 1000) = Chill matrix ğŸ˜
    â€¢ High (> 10^10) = Drama queen matrix ğŸ˜±
    â€¢ Measures "how much small changes in input cause big changes in output"
    
    REAL-WORLD ANALOGY:
    â€¢ Low condition number = Stable friendship (small fights don't break it)
    â€¢ High condition number = High school drama (one rumor ruins everything)
    
    Created by: @kira-ml (GitHub ML Student)
    """
    print("\n" + "ğŸ¬" * 30)
    print("EPISODE 1: STABLE VS. DRAMATIC MATRICES")
    print("ğŸ¬" * 30)
    
    # Scene 1: The Chill Matrix ğŸ˜
    print("\nğŸ¯ SCENE 1: THE CHILL MATRIX (Well-conditioned)")
    print("   Like a stable relationship - small fights don't break it!")
    
    A1 = np.array([[2, 1], [1, 2]])  # Nice, stable matrix
    b1 = np.array([3, 3])
    
    cond_A1 = np.linalg.cond(A1)
    x1 = np.linalg.solve(A1, b1)
    
    print(f"\nğŸ“Š Matrix A (Chill):")
    print(A1)
    print(f"ğŸ§® Condition number: {cond_A1:.2f} (Low drama! ğŸ˜)")
    print(f"ğŸ¯ True solution x: {x1}")
    
    # Add some drama (small perturbation)
    print("\nğŸ­ ADDING A LITTLE DRAMA...")
    b1_perturbed = b1 + np.array([0.01, -0.01])  # Tiny change
    x1_perturbed = np.linalg.solve(A1, b1_perturbed)
    error1 = np.linalg.norm(x1_perturbed - x1) / np.linalg.norm(x1)
    
    print(f"ğŸ“ˆ New b (with drama): {b1_perturbed}")
    print(f"ğŸ¯ New solution: {x1_perturbed}")
    print(f"âš ï¸  Relative error: {error1:.2%}")
    print("ğŸ’¡ Insight: Small input change â†’ Small output change (Good!)")
    
    # Scene 2: The Drama Queen Matrix ğŸ‘‘
    print("\n\nğŸ¯ SCENE 2: THE DRAMA QUEEN MATRIX (Ill-conditioned)")
    print("   Like high school drama - one rumor ruins everything! ğŸ˜±")
    
    n = 5
    A2 = hilbert(n)  # Famous for being dramatic!
    x_true = np.ones(n)
    b2 = A2 @ x_true
    
    cond_A2 = np.linalg.cond(A2)
    x2 = np.linalg.solve(A2, b2)
    
    print(f"\nğŸ“Š Matrix A (Hilbert Matrix - Professional Drama Queen):")
    print(A2)
    print(f"ğŸ§® Condition number: {cond_A2:.2e} (OMG SO DRAMATIC! ğŸ˜±)")
    print(f"ğŸ¯ True solution x: {x_true}")
    print(f"ğŸ¯ Computed solution: {x2}")
    print(f"âš ï¸  Relative error: {np.linalg.norm(x2 - x_true) / np.linalg.norm(x_true):.2%}")
    print("ğŸ’¡ Insight: Perfect input â†’ Still gets wrong answer!")
    
    # Scene 3: The "Almost Twins" Matrix ğŸ‘¯
    print("\n\nğŸ¯ SCENE 3: THE 'ALMOST TWINS' MATRIX (Nearly Singular)")
    print("   Like two nearly identical people - hard to tell apart!")
    
    A3 = np.array([[1, 1], [1, 1.0001]])  # Almost identical rows
    b3 = np.array([2, 2.0001])
    
    cond_A3 = np.linalg.cond(A3)
    x3 = np.linalg.solve(A3, b3)
    
    print(f"\nğŸ“Š Matrix A (Almost Twins):")
    print(A3)
    print(f"ğŸ§® Condition number: {cond_A3:.2f} (High drama alert!)")
    print(f"ğŸ¯ Solution x: {x3}")
    
    # Add microscopic drama
    print("\nğŸ­ ADDING MICROSCOPIC DRAMA...")
    b3_perturbed = b3 + np.array([0.001, 0])  # SUPER tiny change
    x3_perturbed = np.linalg.solve(A3, b3_perturbed)
    error3 = np.linalg.norm(x3_perturbed - x3) / np.linalg.norm(x3)
    
    print(f"ğŸ“ˆ New b (micro-drama): {b3_perturbed}")
    print(f"ğŸ¯ New solution: {x3_perturbed}")
    print(f"âš ï¸  Relative error: {error3:.2%}")
    print("ğŸ’¡ Insight: Microscopic input change â†’ MACROSCOPIC output change!")

def condition_number_vs_error():
    """
    ğŸ“ˆ THE DRAMA GRAPH: How Condition Number Creates Chaos
    
    This plot shows why ML engineers fear high condition numbers!
    It's the "butterfly effect" visualization for matrices.
    
    Created by: @kira-ml (GitHub ML Student)
    """
    print("\n\n" + "ğŸ“Š" * 30)
    print("EPISODE 2: THE DRAMA-ERROR CONNECTION")
    print("ğŸ“Š" * 30)
    
    sizes = range(3, 12)
    condition_numbers = []
    relative_errors = []
    
    print("\nğŸ”¬ EXPERIMENT: Growing Hilbert Matrices")
    print("   Hilbert matrices get MORE dramatic as they grow! ğŸ“ˆ")
    
    for n in sizes:
        A = hilbert(n)  # Professional drama queen matrix
        x_true = np.ones(n)
        b = A @ x_true
        
        cond_num = np.linalg.cond(A)
        condition_numbers.append(cond_num)
        
        x_computed = np.linalg.solve(A, b)
        error = np.linalg.norm(x_computed - x_true) / np.linalg.norm(x_true)
        relative_errors.append(error)
        
        print(f"\nğŸ­ n={n}x{n} Hilbert Matrix:")
        print(f"   Condition number: {cond_num:.2e}")
        print(f"   Relative error: {error:.2e}")
        
        if cond_num > 1e10:
            print("   âš ï¸  WARNING: Condition number > 10^10! Epic drama levels!")
        elif cond_num > 1e6:
            print("   âš ï¸  WARNING: Condition number > 10^6! High drama!")
    
    # Create dramatic visualization
    plt.figure(figsize=(14, 6))
    plt.suptitle('ğŸ­ The Condition Number Drama Effect ğŸ­\nCreated by: @kira-ml', 
                 fontsize=14, fontweight='bold')
    
    # Plot 1: Condition Number Growth
    plt.subplot(1, 2, 1)
    plt.semilogy(sizes, condition_numbers, 'r^-', linewidth=3, markersize=10, 
                 label='Drama Level')
    plt.fill_between(sizes, condition_numbers, alpha=0.2, color='red')
    plt.xlabel('Matrix Size (n x n)', fontsize=12)
    plt.ylabel('Condition Number (log scale)', fontsize=12)
    plt.title('ğŸ“ˆ How Drama Grows with Size', fontsize=13, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    # Add drama zones
    plt.axhline(y=1e10, color='red', linestyle='--', alpha=0.5, 
                label='Epic Drama Zone')
    plt.axhline(y=1e6, color='orange', linestyle='--', alpha=0.5,
                label='High Drama Zone')
    
    # Plot 2: Error Explosion
    plt.subplot(1, 2, 2)
    plt.semilogy(sizes, relative_errors, 'bs-', linewidth=3, markersize=10,
                 label='Error Level')
    plt.fill_between(sizes, relative_errors, alpha=0.2, color='blue')
    plt.xlabel('Matrix Size (n x n)', fontsize=12)
    plt.ylabel('Relative Error (log scale)', fontsize=12)
    plt.title('ğŸ’¥ Error Explosion from Drama', fontsize=13, fontweight='bold')
    plt.grid(True, alpha=0.3)
    plt.legend()
    
    plt.tight_layout()
    plt.show()
    
    print("\nğŸ’¡ KEY TAKEAWAY:")
    print("   Bigger Hilbert matrices = MORE drama = MORE error!")
    print("   This is why ML models can fail with correlated features!")

def stability_analysis():
    """
    ğŸ›¡ï¸ ML SUPERWEAPON: Regularization (The Drama Reducer)
    
    Regularization is like giving your ML model chill pills! ğŸ’Š
    It reduces condition number and prevents overfitting drama.
    
    REAL ML EXAMPLE: Ridge Regression adds Î»I to Xáµ€X
    
    Created by: @kira-ml (GitHub ML Student)
    """
    print("\n\n" + "ğŸ›¡ï¸" * 30)
    print("EPISODE 3: THE DRAMA REDUCER (Regularization)")
    print("ğŸ›¡ï¸" * 30)
    
    # Simulate a typical ML dataset with drama (correlated features)
    np.random.seed(42)
    n_samples, n_features = 100, 10
    
    print("\nğŸ¯ SETUP: Simulating ML Features with Drama")
    print("   Creating features that are highly correlated...")
    print("   (Common in real datasets like housing prices)")
    
    X = np.random.randn(n_samples, n_features)
    # Make features 0, 1, 2 highly correlated (DRAMA SOURCE!)
    X[:, 1] = X[:, 0] + 0.01 * np.random.randn(n_samples)
    X[:, 2] = X[:, 0] + 0.02 * np.random.randn(n_samples)
    
    # Normal equation for linear regression: Xáµ€X w = Xáµ€y
    XTX = X.T @ X
    cond_original = np.linalg.cond(XTX)
    
    print(f"\nğŸ“Š ORIGINAL Xáµ€X Matrix:")
    print(f"   Condition number: {cond_original:.2e}")
    
    if cond_original > 1e10:
        print("   ğŸ”´ CRITICAL DRAMA: Condition > 10^10!")
        print("   Model predictions will be UNSTABLE!")
    elif cond_original > 1e6:
        print("   ğŸŸ¡ HIGH DRAMA: Condition > 10^6!")
        print("   Model might overfit to noise!")
    else:
        print("   ğŸŸ¢ LOW DRAMA: Model should be stable!")
    
    # The superhero: REGULARIZATION!
    print("\n\nğŸ¦¸ SUPERHERO ENTERS: REGULARIZATION!")
    print("   Adding Î»I to Xáµ€X (Ridge Regression trick)")
    print("   Î» = regularization strength (drama reducer power)")
    
    lambda_vals = [0, 1e-8, 1e-6, 1e-4, 1e-2]
    
    print("\n" + "ğŸ§ª" * 50)
    print("EXPERIMENT: How Î» Reduces Drama")
    print("Î» (lambda)\tCondition Number\tDrama Reduction\tEffect")
    print("-" * 60)
    
    for lambda_val in lambda_vals:
        XTX_regularized = XTX + lambda_val * np.eye(n_features)
        cond_regularized = np.linalg.cond(XTX_regularized)
        improvement = cond_original / cond_regularized
        
        # Fun drama rating
        if improvement > 1000:
            drama_effect = "ğŸ­ EPIC CALMING! ğŸ­"
        elif improvement > 100:
            drama_effect = "ğŸ˜Œ Super chill"
        elif improvement > 10:
            drama_effect = "ğŸ˜Š Much calmer"
        elif improvement > 2:
            drama_effect = "ğŸ™‚ A bit calmer"
        else:
            drama_effect = "ğŸ˜ Still dramatic"
        
        print(f"{lambda_val:.0e}\t\t{cond_regularized:.2e}\t\t{improvement:.1f}x\t\t{drama_effect}")
    
    print("\nğŸ’¡ REGULARIZATION TRADE-OFF:")
    print("   More Î» = Less drama (better stability) = Less fitting power")
    print("   Less Î» = More drama (more unstable) = More fitting power")
    print("   Sweet spot usually Î» = 1e-4 to 1e-2 for many ML problems")

def main():
    """
    ğŸš€ MAIN COURSE: Your Journey from Drama to Stability
    
    Welcome to the Condition Number Bootcamp! You'll learn:
    1. What condition number REALLY means for ML
    2. How to spot drama queen matrices
    3. How to fix them with regularization
    
    Created by: @kira-ml (GitHub ML Student)
    Follow my ML journey on GitHub! ğŸ‘©ğŸ’»
    """
    print("\n" + "ğŸŒŸ" * 50)
    print("WELCOME TO: CONDITION NUMBER BOOTCAMP!")
    print("Learn ML's Most Important Stability Concept")
    print("ğŸŒŸ" * 50)
    
    print("\nğŸ‘‹ Hey! I'm Kira, an ML student on GitHub (@kira-ml)")
    print("   I created this tutorial to make numerical stability FUN!")
    
    print("\nğŸ¯ TODAY'S MISSION:")
    print("   â€¢ Episode 1: Spot drama queen matrices ğŸ­")
    print("   â€¢ Episode 2: See the drama-error connection ğŸ“ˆ")
    print("   â€¢ Episode 3: Learn to reduce drama with regularization ğŸ›¡ï¸")
    
    input("\nğŸ¬ Press Enter to start Episode 1...")
    compute_condition_number_demo()
    
    input("\nğŸ“Š Press Enter for Episode 2 (with plots!)...")
    condition_number_vs_error()
    
    input("\nğŸ›¡ï¸ Press Enter for Episode 3 (ML applications!)...")
    stability_analysis()
    
    # Grand Finale!
    print("\n\n" + "ğŸ“" * 50)
    print("CONGRATULATIONS! YOU'VE MASTERED CONDITION NUMBERS!")
    print("ğŸ“" * 50)
    
    print("\nğŸ”¥ YOUR NEW ML SUPERPOWERS:")
    print("   1. ğŸ­ Spot drama queen matrices before they ruin your models")
    print("   2. ğŸ“ˆ Understand why errors explode with high condition numbers")
    print("   3. ğŸ›¡ï¸ Use regularization to stabilize ANY ML model")
    print("   4. ğŸ” Debug why your model gives weird predictions")
    
    print("\nğŸ“š REAL-WORLD APPLICATIONS:")
    print("   â€¢ Linear/Logistic Regression â†’ Check Xáµ€X condition number")
    print("   â€¢ Neural Networks â†’ High condition numbers cause vanishing/exploding gradients")
    print("   â€¢ Recommendation Systems â†’ Matrix factorization stability")
    print("   â€¢ Computer Vision â†’ Numerical stability in transformations")
    
    print("\nğŸ‘©ğŸ’» NEXT STEPS FOR @kira-ml FRIENDS:")
    print("   1. Try sklearn's Ridge() and Lasso() with different Î» values")
    print("   2. Check condition numbers of your own datasets with np.linalg.cond()")
    print("   3. Follow me on GitHub for more beginner-friendly ML tutorials!")
    
    print("\n" + "ğŸ’–" * 50)
    print("Remember: Good ML engineers don't just build models,")
    print("they build STABLE models. You've got this! ğŸ’ª")
    print("ğŸ’–" * 50)
    
    print("\n#MLNewbie #DataScience #Python #NumericalStability")
    print("Created with â¤ï¸ by @kira-ml (GitHub ML Student)")

if __name__ == "__main__":
    # Clear screen for fresh start
    print("\033c", end="")
    main()